{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alex Stewart CS6966\n",
    "## Assign 2\n",
    "### 3/20/2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "np.random.seed(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper methods for part1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(w, x, y):\n",
    "    return sum((np.sign(w.T @ x_) == y_) for x_, y_ in zip(x,y)) / x.shape[0]\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def logistic_regression_grad(w,x_,y_):\n",
    "    z = y_ * w.T @ x_\n",
    "    sig_z = sigmoid(z)\n",
    "    return (sig_z - 1) * x_ * y_\n",
    "\n",
    "def gradient_descent(x, y, w, grad_fn, verbose=False, lr=.01, epochs=100):\n",
    "    start_time = time.time()\n",
    "    for _ in range(epochs):\n",
    "        dw = np.zeros_like(w)\n",
    "\n",
    "        for x_, y_ in zip(x,y):\n",
    "            dw += grad_fn(w,x_,y_)\n",
    "\n",
    "        dw /= x.shape[0]\n",
    "        w -= (lr * dw)\n",
    "\n",
    "    print(f\"Gradient Descent Time: {time.time() - start_time}\")\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1 c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent Time: 0.09143829345703125\n",
      "w: [1.60568358]\n",
      "Percent correct: 100.0%\n"
     ]
    }
   ],
   "source": [
    "def part1_c():\n",
    "    x,y = [], []\n",
    "\n",
    "    for i in range(-50, 51):\n",
    "        if i != 0:\n",
    "            y.append(-1 if i < 0 else 1)\n",
    "            x.append(i)\n",
    "\n",
    "    x = np.array(x)[:, None]\n",
    "    y = np.array(y)\n",
    "    w = np.array([-1.0])\n",
    "    w = gradient_descent(x, y, w, logistic_regression_grad, lr=.1, epochs=100)\n",
    "    print(f\"w: {w}\")\n",
    "    print(f\"Percent correct: {compute_accuracy(w,x,y) * 100}%\")\n",
    "\n",
    "part1_c()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1 d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent Time: 0.08991122245788574\n",
      "w: [1.84613836]\n",
      "Percent correct: 90.0%\n"
     ]
    }
   ],
   "source": [
    "def part1_d():\n",
    "    x,y = [], []\n",
    "\n",
    "    for i in range(-50, 51):\n",
    "        if i != 0:\n",
    "            y.append(-1 if i < 0 else 1)\n",
    "            x.append(i)\n",
    "            \n",
    "    x = np.array(x)\n",
    "    x[np.abs(x) >= 46] *= -1\n",
    "    x = x[:, None]\n",
    "    y = np.array(y)\n",
    "    w = np.array([-1.0])\n",
    "    w = gradient_descent(x, y, w, logistic_regression_grad, lr=.1, epochs=100)\n",
    "    print(f\"w: {w}\")\n",
    "    print(f\"Percent correct: {compute_accuracy(w,x,y) * 100}%\")\n",
    "\n",
    "part1_d()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of A: (1000, 500), Min of A: -0.999992428359048, Max of A: 0.9999920214274678\n",
      "Shape of x_star: (500, 1), Min of x_star: -0.9994751421483206, Max of x_star: 0.9939336051825192\n",
      "Shape of b: (1000, 1)\n",
      "Shape of eta: (1000, 1), mean of eta: 0.03914624213196544, var of eta: 0.5030394187377333\n"
     ]
    }
   ],
   "source": [
    "def part3_c(verbose=False):\n",
    "    n = 500\n",
    "    m = 2 * n\n",
    "    A = np.random.rand(m, n) * 2 - 1\n",
    "    x_star = np.random.rand(n, 1) * 2 - 1\n",
    "    eta = np.random.randn(m, 1) * np.sqrt(.5)\n",
    "    b = A @ x_star + eta    \n",
    "    if verbose:\n",
    "        print(f\"Shape of A: {A.shape}, Min of A: {np.min(A)}, Max of A: {np.max(A)}\")\n",
    "        print(f\"Shape of x_star: {x_star.shape}, Min of x_star: {np.min(x_star)}, Max of x_star: {np.max(x_star)}\")\n",
    "        print(f\"Shape of b: {b.shape}\")\n",
    "        print(f\"Shape of eta: {eta.shape}, mean of eta: {eta.mean()}, var of eta: {eta.var()}\")\n",
    "    return A, x_star, b[:, 0]\n",
    "\n",
    "_, _, _ = part3_c(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper methods for Part 3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_gradient_descent(A, b, grad_fn, verbose=False, lr=.01, epochs=100):\n",
    "    start_time = time.time()\n",
    "    x = np.zeros(A.shape[1])\n",
    "    for _ in range(epochs):\n",
    "        dx = grad_fn(x,A,b)\n",
    "        dx /= A.shape[1]\n",
    "        x -= (lr * dx)\n",
    "\n",
    "    print(f\"Gradient Descent Time: {round(time.time() - start_time, 5)}\")\n",
    "    return x\n",
    "    \n",
    "def l2_linear_regression_grad(x,A,b):\n",
    "    return 2 * A.T @ (A @ x - b)\n",
    "\n",
    "def linear_regression_vector_distance(x_star, x):\n",
    "    return np.linalg.norm(x_star - x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent Time: 0.098\n",
      "Distance between x* and x: 2.21103\n",
      "f(x) = 450.73322, where f(x) is the squared L2 norm of the difference between A * x and b.\n"
     ]
    }
   ],
   "source": [
    "def part3_d(verbose=False):\n",
    "    A, x_star, b = part3_c()\n",
    "    x = linear_regression_gradient_descent(A, b, l2_linear_regression_grad, lr=.1, epochs=50)\n",
    "    x_dist = linear_regression_vector_distance(x_star, x[:, None])\n",
    "    if verbose:\n",
    "        print(f\"Distance between x* and x: {round(x_dist, 5)}\")\n",
    "        print(f\"f(x) = {round(np.linalg.norm(A @ x - b) ** 2, 5)}, where f(x) is the squared L2 norm of the difference between A * x and b.\")\n",
    "    return A, x, x_star, b\n",
    "\n",
    "_, _, _, _ = part3_d(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper function for Part 3e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closed_form_Linear_Regression(A, b):\n",
    "    return np.linalg.inv(A.T @ A) @ A.T @ b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent Time: 0.09864\n",
      "Closed Form Time: 0.50535\n",
      "Distance between closed form x and x*: 1.17938\n",
      "f(x) = 243.45121, where f(x) is the squared L2 norm of the difference between A * x and b.\n"
     ]
    }
   ],
   "source": [
    "def part3_e():\n",
    "    A, _, x_star, b = part3_d()\n",
    "\n",
    "    start_time = time.time()\n",
    "    x_star_closed = closed_form_Linear_Regression(A, b)\n",
    "    print(f\"Closed Form Time: {round(time.time() - start_time, 5)}\")\n",
    "    x_dist = linear_regression_vector_distance(x_star_closed[:, None], x_star)\n",
    "    print(f\"Distance between closed form x and x*: {round(x_dist, 5)}\")\n",
    "    \n",
    "    print(f\"f(x) = {round(np.linalg.norm(A @ x_star_closed - b) ** 2, 5)}, where f(x) is the squared L2 norm of the difference between A * x and b.\")\n",
    "\n",
    "part3_e()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions for Part 4c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def part4_fn(x,y,n,a,b):\n",
    "    total = 0\n",
    "    for i in range(n * 2):\n",
    "        total += (x - a[i])**2 + (y - b[i])**2\n",
    "\n",
    "    return total / (2 * n)\n",
    "\n",
    "def stochastic_gradient_descent(a, b, lr_fn, epochs=100):\n",
    "    x_t, y_t = 1, 1\n",
    "\n",
    "    for t in range(epochs):\n",
    "        lr = lr_fn(t)\n",
    "        i = np.random.randint(0, a.shape[0])\n",
    "        a_, b_ = a[i], b[i]\n",
    "        x_t -= 2 * lr * (x_t - a_)\n",
    "        y_t -= 2 * lr * (y_t - b_)\n",
    "                    \n",
    "    return x_t, y_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 4c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate: .1, x: 0.49521, y: -0.23513, f(x_t,y_t): 1.13865\n",
      "learning rate: .1/(t+1), x: 0.62783, y: 0.48607, f(x_t,y_t): 1.33568\n",
      "learning rate: .1/sqrt(t+1), x: 0.4746, y: 0.0034, f(x_t,y_t): 1.08404\n"
     ]
    }
   ],
   "source": [
    "def part4_c():\n",
    "    n = 500\n",
    "    a, b = [], []\n",
    "    for i in range(1, n + 1):\n",
    "        a.append(i / n)\n",
    "        b.append(-1)\n",
    "    for i in range(n+1, 2*n + 1):\n",
    "        a.append((i - n) / n)\n",
    "        b.append(1)\n",
    "        \n",
    "\n",
    "    lr_fn = lambda t: .1\n",
    "    x, y = stochastic_gradient_descent(np.array(a), np.array(b), lr_fn, epochs=200)\n",
    "    print(f\"learning rate: .1, x: {round(x, 5)}, y: {round(y, 5)}, f(x_t,y_t): {round(part4_fn(x,y,n,a,b), 5)}\")\n",
    "\n",
    "    lr_fn = lambda t: .1 / (t + 1)\n",
    "    x, y = stochastic_gradient_descent(np.array(a), np.array(b), lr_fn, epochs=200)\n",
    "    print(f\"learning rate: .1/(t+1), x: {round(x, 5)}, y: {round(y, 5)}, f(x_t,y_t): {round(part4_fn(x,y,n,a,b), 5)}\")\n",
    "\n",
    "    lr_fn = lambda t: .1 / np.sqrt(t + 1)\n",
    "    x, y = stochastic_gradient_descent(np.array(a), np.array(b), lr_fn, epochs=200)\n",
    "    print(f\"learning rate: .1/sqrt(t+1), x: {round(x, 5)}, y: {round(y, 5)}, f(x_t,y_t): {round(part4_fn(x,y,n,a,b), 5)}\")\n",
    "    \n",
    "\n",
    "part4_c()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "76d49acc42e2e5ddd2c0a2595aee3f474f24d07098dadae8ec45d405aee15672"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('ds')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
